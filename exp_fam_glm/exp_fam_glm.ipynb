{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exponential Family and Generalized Linear Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exponential Family\n",
    "\n",
    "\n",
    "The exponential family is a set of probability distributions with very useful properties. A distribution from the exponential family has the form of:\n",
    "\n",
    "$$p(y \\ | \\ \\boldsymbol{\\theta}) = h(y) \\exp \\Big( \\displaystyle\\sum_i^s \\eta_i (\\boldsymbol{\\theta}) T_i(y) - A(\\boldsymbol{\\theta})\\Big) $$\n",
    "\n",
    "This distribution can be simplified by writing it in its canonical form by setting $\\boldsymbol{\\eta} (\\boldsymbol{\\theta}) = \\boldsymbol{\\theta}$:\n",
    "\n",
    "$$p(y \\ | \\ \\boldsymbol{\\eta}) = h(y) \\exp \\Big( \\displaystyle\\sum_i^s \\eta_i T_i(y) - A(\\boldsymbol{\\eta})\\Big) $$\n",
    "\n",
    "where the $T_i(y)$ are the sufficient statistics, $A(\\boldsymbol{\\eta})$ is the log-normalizer, $h(y)$ is the base-measure and $\\eta_i$ are the natural parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popular Examples\n",
    "\n",
    "### Bernoulli\n",
    "\n",
    "$$p(y \\ | \\ \\pi) = \\pi^{y} \\ (1-\\pi)^{1-y}$$\n",
    "\n",
    "where $y \\in \\{0, 1\\}$ and $\\pi \\in [0, 1]$.\n",
    "\n",
    "Take the exponential of the log density:\n",
    "\n",
    "\\begin{align}\n",
    "p(y \\ | \\ \\pi) &= \\exp \\Big( \\ln \\big(  \\pi^{y} \\ (1-\\pi)^{1-y} \\big) \\Big) \\\\\n",
    "& = \\exp \\Big( y \\ln (\\pi) + (1-y) \\ln(1-\\pi) \\Big) \\\\\n",
    "& = \\exp \\Big( y \\ln (\\frac{\\pi}{1-\\pi}) +  \\ln(1-\\pi) \\Big) \\\\\n",
    "\\end{align}\n",
    "\n",
    "* Sufficient statistic $T(y) = y$\n",
    "* Base measure $h(y) = 1$\n",
    "* Natural parameter $\\eta = \\ln (\\frac{\\pi}{1-\\pi})$\n",
    "* Log-normalizer $A(\\eta) = -\\ln(1-\\pi) = \\ln(1 + e^\\eta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian\n",
    "\n",
    "$$p(y \\ | \\ \\mu, \\ \\sigma) = \\frac{1}{\\sqrt{2 π \\sigma^2}} \\exp\\big({-\\frac{(y - \\mu)^2}{2 \\sigma^2}}\\big)$$\n",
    "\n",
    "where $y \\in \\mathbb{R}$, $\\mu \\in \\mathbb{R}$ and $\\sigma \\in \\mathbb{R}$.\n",
    "\n",
    "Take the exponential of the log density:\n",
    "\n",
    "\\begin{align}\n",
    "p(y \\ | \\ \\mu, \\ \\sigma) &= \\exp \\Big( \\ln \\big( \\frac{1}{\\sqrt{2 π \\sigma^2}}\\big) - \\frac{(y - \\mu)^2}{2 \\sigma^2}  \\Big)\\\\\n",
    "& = \\frac{1}{\\sqrt{2 π}} \\exp \\Big( \\frac{\\mu}{\\sigma^2} y - \\frac{1}{2 \\sigma^2} y^2 - \\frac{\\mu^2}{2 \\sigma^2}  - \\ln(\\sigma) \\Big)\n",
    "\\end{align}\n",
    "\n",
    "* Sufficient statistic $\\mathbf{T}(y) = [y, \\ y^2]^T$\n",
    "* Base measure $h(y) = \\frac{1}{\\sqrt{2 π}}$\n",
    "* Natural parameter $\\boldsymbol{\\eta} = [\\frac{\\mu}{\\sigma^2}, \\ \\frac{-1}{2 \\sigma^2}]^T$\n",
    "* Log-normalizer $A(\\eta) = \\frac{\\mu^2}{2 \\sigma^2} + \\ln(\\sigma)= -\\frac{\\eta_1^2}{4 \\eta_2} - \\frac{1}{2}\\ln(-2\\eta_2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moments of Log-Normalizer\n",
    "\n",
    "$$A(\\boldsymbol{\\eta}) = \\ln \\int h(y) \\exp \\Big( \\displaystyle\\sum_i^s \\eta_i T_i(y) \\Big) dy $$\n",
    "\n",
    "Taking the derivative w.r.t. $\\eta_i$:\n",
    "\n",
    "$$\\frac{\\partial A(\\boldsymbol{\\eta})}{\\partial \\eta_i} = \\frac{\\partial}{\\partial \\eta_i} \\ln \\int h(y) \\exp \\Big( \\displaystyle\\sum_i^s \\eta_i T_i(y) \\Big) dy = \\mathop{\\mathbb{E}}[T_i(y)]$$\n",
    "\n",
    "Taking the second-derivative gives:\n",
    "\n",
    "$$\\frac{\\partial^2 A(\\boldsymbol{\\eta})}{\\partial \\eta_j \\partial \\eta_i} = \\text{Cov}(T_i(y), T_j(y))$$\n",
    "\n",
    "The Hessian matrix of $A(\\boldsymbol{\\eta})$ is positive semi-definite. Proof:\n",
    "\n",
    "$$\\frac{\\partial^2 A(\\boldsymbol{\\eta})}{\\partial \\eta_j \\partial \\eta_i} = \\text{Cov}(T_i(y), T_j(y)) = \\mathop{\\mathbb{E}}[(T_i(y) - \\mathop{\\mathbb{E}}[T_i(y)])(T_j(y) - \\mathop{\\mathbb{E}}[T_j(y)])]$$\n",
    "\n",
    "For simplicity, let $C_{ij}$ = $\\text{Cov}(T_i(y), T_j(y))$ and $v_i$ = $T_i(y) - \\mathop{\\mathbb{E}}[T_i(y)]$:\n",
    "\n",
    "$$C_{ij} = \\mathop{\\mathbb{E}}[v_i v_j]$$\n",
    "\n",
    "For any vector $ \\mathbf{u}$:\n",
    "\n",
    "$$ \\mathbf{u}^T \\mathbf{C} \\mathbf{u} = \\sum_{ij} u_i C_{ij} u_j = \\sum_{ij} u_i\\mathop{\\mathbb{E}}[v_i v_j]u_j = \\mathop{\\mathbb{E}}[\\sum_{ij} u_iv_i v_ju_j] = \\mathop{\\mathbb{E}}[\\sum_{i} u_i v_i \\sum_{j} u_j v_j] = \\mathop{\\mathbb{E}}[(\\sum_{i} u_i v_i)^2] \\geq 0 $$\n",
    "\n",
    "Thus $A(\\boldsymbol{\\eta})$ is a convex function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Estimation\n",
    "\n",
    "The likelihood function for $n$ samples drawn i.i.d. from an exponential distribution:\n",
    "$$p(y_1, y_2, ..., y_n \\ | \\ \\boldsymbol{\\eta}) = \\prod_j^n h(y_j) \\exp \\Big( \\displaystyle\\sum_i^s \\eta_i T_i(y_j) - A(\\boldsymbol{\\eta})\\Big) = \\exp \\Big( \\displaystyle\\sum_i^s \\eta_i \\displaystyle\\sum_j^n T_i(y_j) - n A(\\boldsymbol{\\eta})\\Big) \\prod_j^n h(y_j) $$\n",
    "\n",
    "Taking the negative log of the likelihood:\n",
    "\n",
    "$$\\mathcal{L} = n A(\\boldsymbol{\\eta}) - \\displaystyle\\sum_i^s \\eta_i \\displaystyle\\sum_j^n T_i(y_j) - \\sum_j^n \\ln h(y_j) $$\n",
    "\n",
    "The negative log-likelihood is convex as its Hessian is proportional to the Hessian of $A(\\boldsymbol{\\eta})$:\n",
    "\n",
    "$$\\frac{\\partial^2 \\mathcal{L}}{\\partial \\eta_j \\partial \\eta_i} = n  \\frac{\\partial^2 A(\\boldsymbol{\\eta}) }{\\partial \\eta_j \\partial \\eta_i}$$\n",
    "\n",
    "Therefore, to find the MLE, set the first derivative of the negative log-likelihood to $0$:\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\eta_i} = n  \\frac{\\partial A(\\boldsymbol{\\eta}) }{\\partial \\eta_i} -  \\displaystyle\\sum_j^n T_i(y_j) = 0$$\n",
    "\n",
    "Thus the MLE parameters $\\boldsymbol{\\eta}$ can be determined from the empirical mean of the sufficient statistic:\n",
    "\n",
    "$$\\frac{\\partial A(\\boldsymbol{\\eta}) }{\\partial \\eta_i} = \\frac{1}{n} \\displaystyle\\sum_j^n T_i(y_j)$$\n",
    "\n",
    "For Bernoulli, combining the above equation with previous results will give:\n",
    "\n",
    "$$\\pi_{MLE} = \\frac{1}{n} \\displaystyle\\sum_j^n y_j$$\n",
    "\n",
    "\n",
    "Similarly for Gaussian:\n",
    "\n",
    "$$\\mu_{MLE} = \\frac{1}{n} \\displaystyle\\sum_j^n y_j$$\n",
    "\n",
    "$$\\sigma^2_{MLE} = \\frac{1}{n} \\displaystyle\\sum_j^n (y_j - \\mu_{MLE})^2$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exponential families form the basis for GLMs. The conditional distribution of $y_i$ takes the form: \n",
    "\n",
    "$$p(y_i \\ | \\ \\mathbf{x_i}, \\ \\mathbf{W}) = h(y_i) \\exp \\Big( \\sum_j \\eta_{ij} T_j(y_i) - A(\\boldsymbol{\\eta_i})\\Big) $$\n",
    "\n",
    "where  $\\eta_{ij} = \\sum_{k} X_{ik} W_{jk}$ under the canonical link.\n",
    "\n",
    "$$p(y_i \\ | \\ \\mathbf{x_i}, \\ \\mathbf{W}) = h(y_i) \\exp \\Big( \\sum_j\\sum_{k} X_{ik} W_{jk} T_j(y_i) - A(\\boldsymbol{\\eta_i})\\Big) $$\n",
    "\n",
    "The negative log of the likelihood takes the form:\n",
    "\n",
    "$$\\mathcal{L} = \\sum_{i} A(\\boldsymbol{\\eta_i}) - \\sum_{ijk}  X_{ik} W_{jk} T_j(y_i) - \\sum_i \\ln h(y_i) $$\n",
    "\n",
    "Taking the first derivative gives:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial W_{pq}} = \\sum_{i} \\sum_j \\frac{\\partial A(\\boldsymbol{\\eta_i})}{\\partial \\eta_{ij}} \\frac{\\partial \\eta_{ij}}{\\partial W_{pq}} - \\sum_{i}  X_{iq} T_p(y_i) = \\sum_{i} \\frac{\\partial A(\\boldsymbol{\\eta_i})}{\\partial \\eta_{ip}} X_{iq} - \\sum_{i}  X_{iq} T_p(y_i)$$\n",
    "\n",
    "Taking the second derivative gives:\n",
    "\n",
    "$$\\frac{\\partial^2 \\mathcal{L}}{\\partial W_{mn} \\partial W_{pq}} = \\sum_{i} \\sum_l \\frac{\\partial^2 A(\\boldsymbol{\\eta_i})}{\\partial \\eta_{il} \\partial \\eta_{ip}} \\frac{\\partial \\eta_{il}}{\\partial W_{mn}} X_{iq}  = \\sum_{i} \\frac{\\partial^2 A(\\boldsymbol{\\eta_i})}{\\partial \\eta_{im} \\partial \\eta_{ip}}  X_{in} X_{iq}$$\n",
    "\n",
    "Like previously, this Hessian is positive semi-definite. For any unflattened vector with components $v_{ij}$:\n",
    "\n",
    "$$\\sum_{mnpq} v_{mn} \\frac{\\partial^2 \\mathcal{L}}{\\partial W_{mn} \\partial W_{pq}} v_{pq} =  \\sum_{imnpq} v_{mn} \\frac{\\partial^2 A(\\boldsymbol{\\eta_i})}{\\partial \\eta_{im} \\partial \\eta_{ip}}  X_{in} X_{iq} v_{pq}$$\n",
    "\n",
    "Substitute $\\frac{\\partial^2 A(\\boldsymbol{\\eta_i})}{\\partial \\eta_{im} \\partial \\eta_{ip}} = \\mathop{\\mathbb{E}}[c_{im} c_{ip}]$ like before:\n",
    "\n",
    "$$\\sum_{imnpq} v_{mn} \\frac{\\partial^2 A(\\boldsymbol{\\eta_i})}{\\partial \\eta_{im} \\partial \\eta_{ip}}  X_{in} X_{iq} v_{pq} = \\sum_{imnpq} v_{mn}  \\mathop{\\mathbb{E}}[c_{im} c_{ip}] X_{in} X_{iq} v_{pq} = \\sum_{imnpq} \\mathop{\\mathbb{E}}[ v_{mn} X_{in} X_{iq} v_{pq}] = \\sum_{i} \\mathop{\\mathbb{E}}[ \\sum_{mn} v_{mn} X_{in}  \\sum_{pq} v_{pq}X_{iq} ] \\geq 0 $$\n",
    "\n",
    "Thus the negative log of the likelihood is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "For a GLM with $\\eta = \\mathbf{x}^T\\mathbf{w}$ and $T(y) = y$, likelihood distribution would take the form:\n",
    "\n",
    "\\begin{align}\n",
    "p(y \\ | \\ \\mathbf{x}, \\ \\mathbf{w} ) &=\\exp \\Big( y  \\ \\mathbf{x}^T\\mathbf{w} +  A(\\mathbf{x}, \\mathbf{w}) \\Big) \\\\\n",
    "\\end{align}\n",
    "\n",
    "From previous results, we know that an exponential family of this form will have a log-normalizer $A(\\eta) = \\ln(1 + e^\\eta)$. Thus:\n",
    "\n",
    "\\begin{align}\n",
    "p(y \\ | \\ \\mathbf{x}, \\ \\mathbf{w} ) &=\\exp \\Big( y  \\ \\mathbf{x}^T\\mathbf{w} +  \\ln(1 + e^{\\mathbf{x}^T\\mathbf{w}}) \\Big) \\\\\n",
    "&= \\frac{e^{y  \\ \\mathbf{x}^T\\mathbf{w}}}{1 + e^{\\mathbf{x}^T\\mathbf{w}}}\n",
    "\\end{align}\n",
    "\n",
    "This is the likelihood distribution for logistic regression.\n",
    "\n",
    "### Linear Regression\n",
    "For a GLM with $\\boldsymbol\\eta = [\\mathbf{x}^T\\mathbf{w}, -\\frac{1}{2}]^T$ and $\\mathbf{T}(y) = [y, y^2]^T$, likelihood distribution would take the form:\n",
    "\n",
    "\\begin{align}\n",
    "p(y \\ | \\ \\mathbf{x}, \\ \\mathbf{w} ) = \\frac{1}{\\sqrt{2 π}} \\exp \\Big(y \\ \\mathbf{x}^T \\mathbf{w} -  \\frac{y^2}{2} - A( \\mathbf{x}, \\mathbf{w}) \\Big)\n",
    "\\end{align}\n",
    "\n",
    "From previous results, we know that an exponential family of this form will have a log-normalizer $A(\\eta) = -\\frac{\\eta_1^2}{4 \\eta_2} - \\frac{1}{2}\\ln(-2\\eta_2)$. Thus:\n",
    "\n",
    "\\begin{align}\n",
    "p(y \\ | \\ \\mathbf{x}, \\ \\mathbf{w} ) &= \\frac{1}{\\sqrt{2 π}} \\exp \\Big(y \\ \\mathbf{x}^T \\mathbf{w} -  \\frac{y^2}{2} - \\frac{(\\mathbf{x}^T\\mathbf{w})^2}{2} \\Big)\\\\\n",
    "&= \\frac{1}{\\sqrt{2 π}} \\exp \\Big(-  \\frac{(y - \\mathbf{x}^T\\mathbf{w})^2}{2} \\Big)\n",
    "\\end{align}\n",
    "\n",
    "This is the likelihood distribution for linear regression.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## References\n",
    "\n",
    "- https://www.stat.berkeley.edu/~mjwain/Fall2012_Stat241a/reader_ch8.pdf\n",
    "- https://en.wikipedia.org/wiki/Exponential_family\n",
    "- https://en.wikipedia.org/wiki/Generalized_linear_model\n",
    "- https://math.stackexchange.com/questions/114072/what-is-the-proof-that-covariance-matrices-are-always-semi-definite\n",
    "- https://www.math.snu.ac.kr/~hichoi/machinelearning/lecturenotes/ExpFam_GLM.pdf\n",
    "- https://felix-clark.github.io/glm-math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
