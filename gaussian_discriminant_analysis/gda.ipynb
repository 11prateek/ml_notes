{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian discriminant analysis (GDA) is a generative model used typically for solving classification problems. In this model, the feature vector $\\boldsymbol{x} \\in \\mathbb{R}^{d \\times 1}$ is assumed to be distributed according to a class-conditional multivariate Gaussian distribution:\n",
    "\n",
    "\\begin{align}\n",
    "p(\\boldsymbol{x} \\ | \\ y=c) &= \\frac{1}{(2 \\pi)^{d/2} |\\boldsymbol{\\Sigma_c}|^{1/2}} \\exp \\Big( - \\frac{1}{2} (\\boldsymbol{x} - \\boldsymbol{\\mu_c})^T \\boldsymbol{\\Sigma_c}^{-1} (\\boldsymbol{x} - \\boldsymbol{\\mu_c}) \\Big)\\\\\n",
    "\\end{align}\n",
    "\n",
    "where $\\boldsymbol{\\mu_c} \\in \\mathbb{R}^{d \\times 1}$ and $\\boldsymbol{\\Sigma_c} \\in \\mathbb{R}^{d \\times d}$ are the mean vector and covariance matrix for class $c$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The joint distribution for GDA:\n",
    "\n",
    "\\begin{align}\n",
    "p(\\boldsymbol{x} ,  \\ y) &= p(\\boldsymbol{x} \\ | \\ y) \\  p(y)\\\\\n",
    "\\end{align}\n",
    "\n",
    "where $p(y)$ is a $k$-dimensional categorical distribution to model the class priors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the GDA Parameters\n",
    "\n",
    "The joint distribution for dataset of $N$ i.i.d. samples:\n",
    "\n",
    "\\begin{align}\n",
    "p(\\boldsymbol{x_1}, \\ y_1, \\ ..., \\ \\boldsymbol{x_N}, \\ y_N) &= \\prod_i^N p(\\boldsymbol{x_i}, \\ y_i)\\\\\n",
    "&= \\prod_i^N p(\\boldsymbol{x_i} \\ | \\ y_i) \\ p(y_i)\n",
    "\\end{align}\n",
    "\n",
    "Taking the log of the joint gives:\n",
    "\n",
    "\\begin{align}\n",
    "L &= \\sum_i^N \\ln p(\\boldsymbol{x_i} \\ | \\ y_i) + \\sum_i^N \\ln p(y_i)\\\\\n",
    "&= \\sum_i^N \\sum_c^k \\delta_{y_i, c} \\ln p(\\boldsymbol{x_i} \\ | \\ y_i=c) +  \\sum_i^N \\sum_c^k \\delta_{y_i, c} \\ln p(y_i=c)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the prior\n",
    "\n",
    "Only the second term of the log joint is needed for estimating the prior:\n",
    "\n",
    "\\begin{align}\n",
    "L_{prior} &= \\sum_i^N \\sum_c^k \\delta_{y_i, c} \\ln p(y_i=c)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Let $p(y_i=c) = \\pi_c$. Since the $\\pi_c$ follow a $k$-dimensional categorical distribution, these can be written as:\n",
    "\n",
    "\\begin{align}\n",
    "\\sum_c^k \\pi_c =\\sum_c^{k-1} \\pi_c + \\pi_k= 1\\\\\n",
    "\\end{align}\n",
    "\n",
    "Rearranging gives:\n",
    "\n",
    "$$\\pi_k= 1 - \\sum_c^{k-1} \\pi_c$$\n",
    "\n",
    "Substituting this constraint into the log prior gives:\n",
    "\n",
    "\\begin{align}\n",
    "L_{prior} &= \\sum_i^N \\Big( \\sum_c^{k-1} \\delta_{y_i, c} \\ln \\pi_c +  \\delta_{y_i, k}  \\ln\\big(1 - \\sum_c^{k-1} \\pi_c \\big)\\Big)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Maximizing the log prior by taking derivative:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial L_{prior}}{\\partial \\pi_j} &= \\sum_i^N \\Big( \\sum_c^{k-1} \\delta_{y_i, c} \\delta_{j, c} \\frac{1}{\\pi_c} -  \\delta_{y_i, k}  \\frac{ \\sum_c^{k-1} \\delta_{c, j}}{\\big(1 - \\sum_c^{k-1} \\pi_c \\big)}\\Big)\\\\\n",
    "&= \\sum_i^N \\Big( \\delta_{y_i, j} \\frac{1}{\\pi_j} -  \\delta_{y_i, k}  \\frac{1}{\\big(1 - \\sum_c^{k-1} \\pi_c \\big)}\\Big)\\\\\n",
    "&= \\frac{1}{\\pi_j} \\sum_i^N \\delta_{y_i, j}  - \\frac{1}{\\big(1 - \\sum_c^{k-1} \\pi_c \\big)} \\sum_i^N  \\delta_{y_i, k}  \n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Setting the derivative to $0$ and rearranging gives:\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\pi_j  &= \\big(1 - \\sum_c^{k-1} \\pi_c \\big) \\frac{\\sum_i^N \\delta_{y_i, j}}{\\sum_i^N  \\delta_{y_i, k}  }\\\\\n",
    "&= \\pi_k \\frac{\\sum_i^N \\delta_{y_i, j}}{\\sum_i^N  \\delta_{y_i, k}  }\n",
    "\\end{align}\n",
    "\n",
    "Summing both sides for all $j \\in \\{1, ... k\\}$:\n",
    "\n",
    "\\begin{align}\n",
    "\\sum_j^{k}\\pi_j  &= \\pi_k \\frac{\\sum_i^N \\sum_j^{k} \\delta_{y_i, j}}{\\sum_i^N  \\delta_{y_i, k}  } \\\\\n",
    "&=  \\pi_k \\frac{N}{\\sum_i^N  \\delta_{y_i, k}  }\n",
    "\\end{align}\n",
    "\n",
    "Using $\\sum_j^{k}\\pi_j = 1$ and rearranging:\n",
    "\n",
    "\\begin{align}\n",
    "\\pi_k = \\frac{\\sum_i^N  \\delta_{y_i, k}}{N}\n",
    "\\end{align}\n",
    "\n",
    "This result can be used to determine $\\pi_j$:\n",
    "\n",
    "\\begin{align}\n",
    "\\pi_j = \\frac{\\sum_i^N  \\delta_{y_i, j}}{N}\n",
    "\\end{align}\n",
    "\n",
    "Thus the prior probability $p(y_i = c)$ is simply the number of training samples in class $c$ divided by total number of training samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the Mean\n",
    "\n",
    "Only the first term of the log joint is needed for estimating the mean $\\boldsymbol{\\mu_c}$:\n",
    "\n",
    "\\begin{align}\n",
    "L_{cond} &= \\sum_i^N \\sum_c^k \\delta_{y_i, c} \\ln p(\\boldsymbol{x_i} \\ | \\ y_i=c)\\\\\n",
    "&= - \\frac{1}{2} \\sum_i^N \\sum_c^k \\delta_{y_i, c} \\Big( \\ln{|\\boldsymbol{\\Sigma_c}|} + (\\boldsymbol{x_i} - \\boldsymbol{\\mu_c})^T \\boldsymbol{\\Sigma_c}^{-1} (\\boldsymbol{x_i} - \\boldsymbol{\\mu_c}) \\Big) + const.\\\\\n",
    "&= - \\frac{1}{2} \\sum_i^N \\sum_c^k \\delta_{y_i, c} \\Big( \\ln{|\\boldsymbol{\\Sigma_c}|} + \\boldsymbol{x_i}^T\\boldsymbol{\\Sigma_c}^{-1} \\boldsymbol{x_i} - 2 \\boldsymbol{\\mu_c}^T \\boldsymbol{\\Sigma_c}^{-1} \\boldsymbol{x_i} +  \\boldsymbol{\\mu_c}^T \\boldsymbol{\\Sigma_c}^{-1} \\boldsymbol{\\mu_c} \\Big) + const.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the derivative:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial L_{cond}}{\\partial \\boldsymbol{\\mu_c}} &= \\sum_i^N  \\delta_{y_i, c} \\big( \\boldsymbol{\\Sigma_c}^{-1} \\boldsymbol{x_i} - \\boldsymbol{\\Sigma_c}^{-1} \\boldsymbol{\\mu_c} \\big)\\\\\n",
    "&= \\boldsymbol{\\Sigma_c}^{-1} \\sum_i^N  \\delta_{y_i, c} \\big(  \\boldsymbol{x_i} - \\boldsymbol{\\mu_c} \\big)\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the derivative to $0$ and rearranging gives:\n",
    "    \n",
    "\\begin{align}\n",
    "\\boldsymbol{\\mu_c}  &=  \\frac{\\sum_i^N  \\delta_{y_i, c}   \\boldsymbol{x_i}}{\\sum_i^N  \\delta_{y_i, c}}\\\\\n",
    "\\end{align}\n",
    "\n",
    "Thus $\\boldsymbol{\\mu_c}$ is simply the mean of the feature vector for all samples from class $c$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the Covariance \n",
    "\n",
    "Once again, only the first term of the log joint is needed for estimating the covariance $\\boldsymbol{\\Sigma_c}$:\n",
    "\n",
    "\\begin{align}\n",
    "L_{cond} &= - \\frac{1}{2} \\sum_i^N \\sum_c^k \\delta_{y_i, c} \\Big( \\ln{|\\boldsymbol{\\Sigma_c}|} + (\\boldsymbol{x_i} - \\boldsymbol{\\mu_c})^T \\boldsymbol{\\Sigma_c}^{-1} (\\boldsymbol{x_i} - \\boldsymbol{\\mu_c}) \\Big) + const.\\\\\n",
    "\\end{align}\n",
    "\n",
    "Taking the derivative w.r.t. $\\boldsymbol{\\Sigma_c}^{-1}$ using $\\frac{\\partial \\boldsymbol{v^T \\Lambda v}}{\\partial \\boldsymbol{\\Lambda}} = \\boldsymbol{vv^T}$ and $\\frac{\\partial \\ln |\\boldsymbol{\\Lambda}^{-1}|}{\\partial \\boldsymbol{\\Lambda}} = - (\\boldsymbol{\\Lambda}^{-1})^T$:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac {\\partial L_{cond}}{\\partial \\boldsymbol{\\Sigma_c}^{-1}} &=  \\frac{1}{2} \\sum_i^N \\delta_{y_i, c} \\Big( \\boldsymbol{\\Sigma_c} - (\\boldsymbol{x_i} - \\boldsymbol{\\mu_c})(\\boldsymbol{x_i} - \\boldsymbol{\\mu_c})^T \\Big)\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the derivative to $0$ and rearranging gives:\n",
    "\n",
    "\\begin{align}\n",
    "\\boldsymbol{\\Sigma_c} &=  \\frac{\\sum_i^N \\delta_{y_i, c} (\\boldsymbol{x_i} - \\boldsymbol{\\mu_c})(\\boldsymbol{x_i} - \\boldsymbol{\\mu_c})^T}{\\sum_i^N \\delta_{y_i, c}}\\\\\n",
    "\\end{align}\n",
    "\n",
    "Thus $\\boldsymbol{\\Sigma_c}$ is the covariance matrix computed on all samples from class $c$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis\n",
    "\n",
    "Linear discriminant analysis (LDA) is a special case where the covariance matrix is the same across each class:\n",
    "\n",
    "\\begin{align}\n",
    "p(\\boldsymbol{x} \\ | \\ y=c) &= \\frac{1}{(2 \\pi)^{d/2} |\\boldsymbol{\\Sigma}|^{1/2}} \\exp \\Big( - \\frac{1}{2} (\\boldsymbol{x} - \\boldsymbol{\\mu_c})^T \\boldsymbol{\\Sigma}^{-1} (\\boldsymbol{x} - \\boldsymbol{\\mu_c}) \\Big)\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The covariance matrix is estimated in a slightly different way:\n",
    "\n",
    "\\begin{align}\n",
    "L_{cond} &= - \\frac{N}{2} \\ln{|\\boldsymbol{\\Sigma}|} - \\frac{1}{2} \\sum_i^N \\sum_c^k \\delta_{y_i, c}  (\\boldsymbol{x_i} - \\boldsymbol{\\mu_c})^T \\boldsymbol{\\Sigma}^{-1} (\\boldsymbol{x_i} - \\boldsymbol{\\mu_c})  + const.\\\\\n",
    "\\end{align}\n",
    "\n",
    "Taking the derivative w.r.t. $\\boldsymbol{\\Sigma}^{-1}$:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac {\\partial L_{cond}}{\\partial \\boldsymbol{\\Sigma}^{-1}} &= \\frac{N}{2} \\boldsymbol{\\Sigma} - \\frac{1}{2} \\sum_i^N \\sum_c^k  \\delta_{y_i, c} (\\boldsymbol{x_i} - \\boldsymbol{\\mu_c})(\\boldsymbol{x_i} - \\boldsymbol{\\mu_c})^T \\\\\n",
    "\\end{align}\n",
    "\n",
    "Setting the derivative to $0$ and rearranging gives:\n",
    "\n",
    "\\begin{align}\n",
    "\\boldsymbol{\\Sigma} &= \\frac{1}{N} \\sum_i^N \\sum_c^k  \\delta_{y_i, c} (\\boldsymbol{x_i} - \\boldsymbol{\\mu_c})(\\boldsymbol{x_i} - \\boldsymbol{\\mu_c})^T \\\\\n",
    "&= \\frac{1}{N}  \\sum_c^k \\sum_i^N  \\delta_{y_i, c} (\\boldsymbol{x_i} - \\boldsymbol{\\mu_c})(\\boldsymbol{x_i} - \\boldsymbol{\\mu_c})^T \\\\\n",
    "&= \\frac{1}{N}  \\sum_c^k \\sum_i^N  \\delta_{y_i, c} \\boldsymbol{\\Sigma}_c\\\\\n",
    "&= \\frac{1}{N}  \\sum_c^k \\boldsymbol{\\Sigma}_c \\sum_i^N  \\delta_{y_i, c} \n",
    "\\end{align}\n",
    "\n",
    "This can be interpreted as weighted average of the covariance matrices from the general case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References \n",
    "\n",
    "- https://online.stat.psu.edu/stat508/lesson/9/9.2\n",
    "- https://statisticaloddsandends.wordpress.com/2018/05/24/derivative-of-log-det-x/\n",
    "- https://stats.stackexchange.com/questions/129062/what-is-the-correct-formula-for-covariance-matrix-in-quadratic-discriminant-anal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
